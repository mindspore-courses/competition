# ğŸš€ [ALL IN ONE]
## æ¨¡å‹å¾®è°ƒèµ›
https://chi-2024.obs.cn-southwest-2.myhuaweicloud.com/2024-llm-stage2/

## ğŸ“‘ ç›®å½•

1. [å¾®è°ƒç®—æ³•ä»‹ç»](#1-å¾®è°ƒç®—æ³•ä»‹ç»)
2. [å¾®è°ƒå…¨æµç¨‹å®ç°](#2-å¾®è°ƒå…¨æµç¨‹å®ç°)
3. [ç¯å¢ƒé…ç½®è¯¦æƒ…](#3-ç¯å¢ƒé…ç½®è¯¦æƒ…)
4. [æ•°æ®é›†å¤„ç†æ–¹æ³•](#4-æ•°æ®é›†å¤„ç†æ–¹æ³•)
5. [æ¨¡å‹é…ç½®ä¸å‚æ•°](#5-æ¨¡å‹é…ç½®ä¸å‚æ•°)
6. [è®­ç»ƒä¸è¯„ä¼°](#6-è®­ç»ƒä¸è¯„ä¼°)
7. [æ¨ç†éƒ¨ç½²](#7-æ¨ç†éƒ¨ç½²)
8. [é™„ä»¶è¯´æ˜](#8-é™„ä»¶è¯´æ˜)

## 1. å¾®è°ƒç®—æ³•ä»‹ç»

### ğŸ” LoRA (Low-Rank Adaptation) ç®—æ³•

#### æ ¸å¿ƒåŸç†
LoRAé€šè¿‡åœ¨é¢„è®­ç»ƒæ¨¡å‹ä¸­æ³¨å…¥å¯è®­ç»ƒçš„ä½ç§©çŸ©é˜µæ¥å®ç°é«˜æ•ˆå¾®è°ƒï¼š

```python
Î”W = BA  # å…¶ä¸­Bâˆˆâ„^{dÃ—r}, Aâˆˆâ„^{rÃ—k}
```

- é¢„è®­ç»ƒæƒé‡ä¿æŒå†»ç»“
- ä»…è®­ç»ƒä½ç§©çŸ©é˜µAå’ŒB
- ç§©ré€šå¸¸è¿œå°äºåŸå§‹ç»´åº¦ï¼ˆå¦‚r=8,16ï¼‰

#### å®ç°é…ç½®
æ ¹æ®é…ç½®æ–‡ä»¶ï¼Œæœ¬é¡¹ç›®LoRAå‚æ•°å¦‚ä¸‹ï¼š
```yaml
pet_config:
  pet_type: lora
  lora_rank: 8
  lora_alpha: 16
  lora_dropout: 0.05
  target_modules: '.*wq|.*wk|.*wv|.*wo'
```

#### ä¼˜åŒ–ç­–ç•¥
- ä½¿ç”¨Adamä¼˜åŒ–å™¨
- ä½™å¼¦å­¦ä¹ ç‡è°ƒåº¦
- åŠ¨æ€æŸå¤±ç¼©æ”¾
- æ¢¯åº¦è£å‰ª

## 2. å¾®è°ƒå…¨æµç¨‹å®ç°

### 2.1 åŸºæœ¬ä¿¡æ¯
- åŸºåº§æ¨¡å‹ï¼šInternLM-7B 
- å¾®è°ƒæ–¹æ³•ï¼šLoRA
- ç›®æ ‡ä»»åŠ¡ï¼šæå‡é€‰æ‹©é¢˜å‡†ç¡®ç‡
- è¯„ä¼°æŒ‡æ ‡ï¼šä½å‚æ¯”ä¾‹ + å‡†ç¡®ç‡

### 2.2 å®ç°æµç¨‹

```mermaid
graph TD
    A[ç¯å¢ƒå‡†å¤‡] --> B[æ•°æ®å‡†å¤‡]
    B --> C[æ¨¡å‹é…ç½®]
    C --> D[è®­ç»ƒå¯åŠ¨]
    D --> E[æ¨¡å‹è¯„ä¼°]
    E --> F[æƒé‡åˆå¹¶]
    F --> G[ç»“æœæ¨ç†]
```

## 3. ç¯å¢ƒé…ç½®è¯¦æƒ…

### 3.1 ç¡¬ä»¶ç¯å¢ƒ
- è®¡ç®—å¹³å°ï¼šåä¸ºäº‘ModelArts
- è®¡ç®—èµ„æºï¼š4å¡NPUï¼ˆAscend 910Bï¼‰
- å†…å­˜é…ç½®ï¼š64GBæ˜¾å­˜/å¡
- å­˜å‚¨ç©ºé—´ï¼š500GB

### 3.2 è½¯ä»¶ç¯å¢ƒ
```bash
# åŸºç¡€ç¯å¢ƒ
MindSpore==2.3.0RC2
MindFormers=={specific_version}
tiktoken
```

## 4. æ•°æ®é›†å¤„ç†æ–¹æ³•

### 4.1 æ•°æ®é›†ä¿¡æ¯
1. MMLUæ•°æ®é›†
2. CMMLUæ•°æ®é›†
3. SQUADæ•°æ®é›†ï¼ˆè¯„ä¼°ç”¨ï¼‰

### 4.2 å¤„ç†æµç¨‹
1. CSVè½¬JSONï¼ˆAlpacaæ ¼å¼ï¼‰
```python
{
    "instruction": "æ ¹æ®é—®é¢˜é€‰æ‹©æ­£ç¡®ç­”æ¡ˆ",
    "input": "é—®é¢˜å†…å®¹",
    "output": "æ­£ç¡®é€‰é¡¹"
}
```

2. JSONè½¬MindRecord
```bash
python transform_dataset.py \
    --input alpaca_format.json \
    --output mindrecord/ \
    --num_shards 4
```

## 5. æ¨¡å‹é…ç½®ä¸å‚æ•°

### 5.1 è®­ç»ƒè¶…å‚æ•°
```yaml
# è®­ç»ƒç›¸å…³
epochs: 10
batch_size: 4
learning_rate: 5.e-5
warmup_ratio: 0.03

# å¹¶è¡Œç­–ç•¥
data_parallel: 4
model_parallel: 1
pipeline_stage: 1
```

### 5.2 ä¼˜åŒ–å™¨é…ç½®
```yaml
optimizer:
  type: FP32StateAdamWeightDecay
  beta1: 0.9
  beta2: 0.999
  eps: 1.e-8
  weight_decay: 0.01
```

### 5.3 è°ƒåº¦å™¨é…ç½®
```yaml
lr_schedule:
  type: CosineWithWarmUpLR
  learning_rate: 5.e-5
  warmup_ratio: 0.03
```

## 6. è®­ç»ƒä¸è¯„ä¼°

### 6.1 è®­ç»ƒå¯åŠ¨
```bash
# 4å¡è®­ç»ƒå¯åŠ¨å‘½ä»¤
bash scripts/msrun_launcher.sh "python research/internlm/run_internlm.py --run_mode finetune --use_parallel True --config research/internlm/finetune_internlm_7b_lora_mmlu_64G.yaml --load_checkpoint /home/ma-user/work/stage2/internlm.ckpt --auto_trans_ckpt True --train_dataset /home/ma-user/work/stage2/mmlu/mmlu.mindrecord" 4
```

### 6.2 æ¨¡å‹è¯„ä¼°
åœ¨SQUADæ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼š
```bash
python run_internlm.py \
--config predict_internlm_7b_eval_squad.yaml \
--run_mode eval \
--load_checkpoint /home/ma-user/work/new_lora_checkpoint_0.ckpt \
--use_parallel False \
--eval_dataset /home/ma-user/work/squad8192.mindrecord > eval_squad.log 2>&1 &
```
- F1 Score: [53.882715209537615]
- EM Score: [31.591678761490083]

### 6.3 å‚æ•°æ•ˆç‡
- æ€»å‚æ•°é‡: 7B
- å¯è®­ç»ƒå‚æ•°é‡: [8.3M]
- å‚æ•°æ¯”ä¾‹: [0.1146]%

## 7. æµ‹è¯•æ¨ç†
```bash
python run_internlm.py \
--config predict_internlm_7b_mmlu.yaml \
--run_mode predict \
--use_parallel false \
--load_checkpoint /home/ma-user/work/stage2/mindformers/output/checkpoint/rank_0/new_lora_checkpoint_0.ckpt \
--auto_trans_ckpt false \
--input_dir /home/ma-user/work/stage2/mmlu/mmlu_alpaca_format2000.json > predict2000.log 2>&1 &
```
### 7.1 æƒé‡åˆå¹¶
```bash
cd /home/ma-user/work/mindformers/

python mindformers/tools/transform_ckpt.py \
--src_ckpt_strategy /home/ma-user/work/mindformers/output/strategy/ \
--src_ckpt_dir /home/ma-user/work/mindformers/output/checkpoint/ \
--dst_ckpt_dir /home/ma-user/work/mindformers/output/checkpoint/ \
--prefix "new_lora_checkpoint_"
```

### 7.2 æ¨ç†é…ç½®
```yaml
model_config:
  batch_size: 8
  max_device_memory: "58GB"
  vocab_file: "path/to/vocab"
```

### 7.3 æ¨ç†æ‰§è¡Œ
```bash
python predict.py \
    --config_path predict_config.yaml \
    --input_file test.json \
    --output_file results.npy
```

## 8. é™„ä»¶è¯´æ˜

### ğŸ“ æ–‡ä»¶æ¸…å•
- âœ… è®­ç»ƒæ—¥å¿—
- âœ… é…ç½®æ–‡ä»¶
    - è®­ç»ƒã€åŸæœ‰èƒ½åŠ›è¯„ä¼°ã€æ¨ç†çš„é…ç½®æ–‡ä»¶ï¼šhttps://chi-2024.obs.cn-southwest-2.myhuaweicloud.com/2024-llm-stage2/config/
- âœ… è¯„ä¼°ç»“æœ
    - https://chi-2024.obs.cn-southwest-2.myhuaweicloud.com/2024-llm-stage2/result/
- âœ… æ¨¡å‹æƒé‡
    - https://chi-2024.obs.cn-southwest-2.myhuaweicloud.com/2024-llm-stage2/lora/
- âœ… Mindformers
    - https://chi-2024.obs.cn-southwest-2.myhuaweicloud.com/2024-llm-stage2/mindformers/mindformers/



---
ğŸ’¡ æ³¨ï¼šæœ¬æŠ¥å‘Šä½¿ç”¨Markdownæ ¼å¼ç¼–å†™ï¼Œæ”¯æŒå¯¼å‡ºä¸ºPDFç­‰å…¶ä»–æ ¼å¼ã€‚æ‰€æœ‰é…ç½®è·¯å¾„å’Œå‚æ•°å‡å¯æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ã€‚