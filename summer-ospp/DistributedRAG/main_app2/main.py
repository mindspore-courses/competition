import os
import time
import logging
import re
from datetime import datetime
from typing import List, Dict, Tuple

import streamlit as st
import ray
from ddgs import DDGS
from bs4 import BeautifulSoup
import requests

from qwen_embedding_model import QwenEmbeddingModel
from qwen_reranker_model import QwenRerankerModel
from qwen_causal_lm import QwenCausalLM
from ray_tasks import parse_and_chunk_document

RAY_ADDRESS = os.getenv("RAY_ADDRESS", "ray://127.0.0.1:10001")
MILVUS_HOST = os.getenv("MILVUS_HOST", "standalone")
MILVUS_PORT = os.getenv("MILVUS_PORT", "19530")
MAX_OPTIMIZATION_ATTEMPTS = 2

EMBEDDING_MODEL_PATH = "/app/.mindnlp/model/Qwen3-Embedding"
RERANKER_MODEL_PATH = "/app/.mindnlp/model/Qwen3-Reranker"
LLM_MODEL_PATH = "/app/.mindnlp/model/Qwen2_5-1_5B-Instruct"
EMBEDDING_DIM = 1024

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

try:
    if not ray.is_initialized():
        logging.info(f"æ­£åœ¨è¿æ¥åˆ° Ray é›†ç¾¤: {RAY_ADDRESS}")
        ray.init(address=RAY_ADDRESS, ignore_reinit_error=True)
    logging.info("âœ… Ray è¿æ¥æˆåŠŸ!")
except Exception as e:
    logging.error(f"âŒ æ— æ³•è¿æ¥åˆ° Ray é›†ç¾¤: {e}")
    st.error(f"ä¸¥é‡é”™è¯¯ï¼šæ— æ³•è¿æ¥åˆ° Ray è®¡ç®—é›†ç¾¤ã€‚é”™è¯¯è¯¦æƒ…: {e}")
    st.stop()

class EmbeddingActor:
    def __init__(self):
        self.model = QwenEmbeddingModel(EMBEDDING_MODEL_PATH)
    def embed(self, texts: List[str]) -> List[List[float]]:
        embeddings = self.model.encode(texts)
        return embeddings.tolist()

class RerankerActor:
    def __init__(self):
        self.model = QwenRerankerModel(RERANKER_MODEL_PATH)
    def compute_score(self, sentence_pairs: List[Tuple[str, str]]) -> List[float]:
        return self.model.compute_score(sentence_pairs)

class LLMActor:
    def __init__(self):
        self.model = QwenCausalLM(LLM_MODEL_PATH)
    def generate(self, prompt: str) -> str:
        messages = [{"role": "user", "content": prompt}]
        return self.model.generate(messages)

class MilvusClient:
    def __init__(self, host, port):
        from pymilvus import connections, utility, FieldSchema, CollectionSchema, DataType, Collection
        self.connections, self.utility, self.Collection = connections, utility, Collection
        self.DataType, self.FieldSchema, self.CollectionSchema = DataType, FieldSchema, CollectionSchema
        for i in range(5):
            try:
                self.connections.connect("default", host=host, port=port)
                logging.info("âœ… Milvus è¿æ¥æˆåŠŸã€‚")
                return
            except Exception as e:
                logging.warning(f"Milvus è¿æ¥å°è¯• {i+1}/5 å¤±è´¥... Error: {e}")
                time.sleep(3)
        raise ConnectionError("é”™è¯¯ï¼šå¤šæ¬¡å°è¯•åæ— æ³•è¿æ¥åˆ°Milvusã€‚")

    def create_or_get_collection(self, collection_name: str, dim: int = EMBEDDING_DIM) -> 'Collection':
        if self.utility.has_collection(collection_name):
            return self.Collection(collection_name)
        fields = [
            self.FieldSchema(name="pk", dtype=self.DataType.VARCHAR, is_primary=True, auto_id=True, max_length=100),
            self.FieldSchema(name="text", dtype=self.DataType.VARCHAR, max_length=65535),
            self.FieldSchema(name="embedding", dtype=self.DataType.FLOAT_VECTOR, dim=dim)
        ]
        schema = self.CollectionSchema(fields, "RAGçŸ¥è¯†åº“é›†åˆ")
        collection = self.Collection(name=collection_name, schema=schema)
        index_params = {"index_type": "IVF_FLAT", "metric_type": "L2", "params": {"nlist": 1024}}
        collection.create_index(field_name="embedding", index_params=index_params)
        return collection

    def insert(self, collection_name: str, texts: List[str], vectors: List[List[float]]):
        collection = self.create_or_get_collection(collection_name)
        collection.insert([texts, vectors])
        collection.flush()

    def search(self, collection_name: str, query_vector: List[List[float]], top_k: int = 10) -> List[str]:
        if not self.utility.has_collection(collection_name):
            return ["é”™è¯¯ï¼šçŸ¥è¯†åº“é›†åˆä¸å­˜åœ¨ã€‚"]
        collection = self.Collection(collection_name)
        collection.load()
        search_params = {"metric_type": "L2", "params": {"nprobe": 10}}
        results = collection.search(data=query_vector, anns_field="embedding", param=search_params, limit=top_k, output_fields=["text"])
        return [hit.entity.get('text') for hit in results[0]] if results else []

RELEVANCE_ASSESSMENT_TEMPLATE = """ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£ç›¸å…³æ€§è¯„ä¼°å‘˜ã€‚è¯·åˆ¤æ–­ä¸‹é¢æä¾›çš„ã€æ–‡æ¡£ç‰‡æ®µã€‘æ˜¯å¦èƒ½å¸®åŠ©å›ç­”ã€ç”¨æˆ·é—®é¢˜ã€‘ã€‚è¯·åªå›ç­”â€œæ˜¯â€æˆ–â€œå¦â€ã€‚ã€ç”¨æˆ·é—®é¢˜ã€‘\n{question}\n\nã€æ–‡æ¡£ç‰‡æ®µã€‘\n---\n{document}\n---\n\nã€è¯¥æ–‡æ¡£æ˜¯å¦ç›¸å…³ï¼Ÿã€‘"""
QUERY_OPTIMIZATION_TEMPLATE = """ä½ æ˜¯ä¸€ä¸ªæœç´¢å¼•æ“ä¼˜åŒ–ä¸“å®¶ã€‚å½“å‰çš„ç”¨æˆ·é—®é¢˜åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ£€ç´¢åˆ°ç›¸å…³çš„ç»“æœã€‚è¯·ä½ æ¢ä¸€ä¸ªè§’åº¦ï¼Œä½¿ç”¨ä¸åŒçš„å…³é”®è¯æˆ–è¡¨è¾¾æ–¹å¼ï¼Œé‡æ–°ç”Ÿæˆä¸€ä¸ªä¸åŸé—®é¢˜æ„å›¾ç›¸åŒï¼Œä½†å¯èƒ½æ›´å®¹æ˜“åœ¨æ•°æ®åº“ä¸­åŒ¹é…åˆ°å†…å®¹çš„æ–°é—®é¢˜ã€‚è¯·åªæä¾›ä¼˜åŒ–åçš„æ–°é—®é¢˜ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€‚ã€åŸå§‹é—®é¢˜ã€‘\n{question}\n\nã€ä¼˜åŒ–åçš„æ–°é—®é¢˜ã€‘"""
FINAL_ANSWER_TEMPLATE = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€ä¸¥è°¨çš„é—®ç­”åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä¸‹é¢æä¾›çš„ã€å¯å‚è€ƒçš„ä¸Šä¸‹æ–‡ã€‘æ¥å›ç­”ç”¨æˆ·çš„ã€é—®é¢˜ã€‘ã€‚ä½ çš„å›ç­”å¿…é¡»éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š1. å®Œå…¨åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡è¿›è¡Œå›ç­”ï¼Œç¦æ­¢ä½¿ç”¨ä»»ä½•å¤–éƒ¨çŸ¥è¯†æˆ–è¿›è¡ŒçŒœæµ‹ã€‚2. åœ¨å›ç­”ä¸­ï¼Œä½ å¿…é¡»æ˜ç¡®å¼•ç”¨ä¿¡æ¯æ¥æºã€‚å¼•ç”¨æ ¼å¼ä¸ºï¼š[æ¥æº: æ–‡ä»¶å (å—å·: X)]ã€‚3. å¦‚æœä¸Šä¸‹æ–‡å†…å®¹è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·æ¸…æ™°ã€å‡†ç¡®åœ°ç»„ç»‡ç­”æ¡ˆã€‚4. å¦‚æœä¸Šä¸‹æ–‡å†…å®¹ä¸ç›¸å…³æˆ–ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºï¼šâ€œæ ¹æ®æ‚¨æä¾›çš„æ–‡æ¡£ï¼Œæˆ‘æ— æ³•æ‰¾åˆ°å…³äºè¿™ä¸ªé—®é¢˜çš„ç¡®åˆ‡ä¿¡æ¯ã€‚â€5. å›ç­”æ—¶è¯·ä¿æŒå®¢è§‚ã€ä¸“ä¸šçš„å£å»ï¼Œå¹¶ä¸”æ€»æ˜¯ä½¿ç”¨ä¸­æ–‡ã€‚ã€å¯å‚è€ƒçš„ä¸Šä¸‹æ–‡ã€‘\n---\n{context}\n---\n\nã€é—®é¢˜ã€‘\n{question}\n\nã€ä½ çš„å›ç­”ã€‘"""
HYDE_PROMPT_TEMPLATE = """ä½ æ˜¯ä¸€ä¸ªå–„äºå›ç­”é—®é¢˜çš„åŠ©æ‰‹ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„ã€é—®é¢˜ã€‘ï¼Œç”Ÿæˆä¸€ä¸ªè¯¦ç»†ã€å®Œæ•´ã€çœ‹èµ·æ¥éå¸¸ä¸“ä¸šçš„å›ç­”ã€‚é‡è¦æç¤ºï¼šè¿™ä¸ªå›ç­”æ˜¯ç”¨äºåç»­æ£€ç´¢çš„ï¼Œæ‰€ä»¥å®ƒä¸éœ€è¦ä¿è¯äº‹å®çš„ç»å¯¹æ­£ç¡®æ€§ï¼Œä½†å¿…é¡»ä¸é—®é¢˜é«˜åº¦ç›¸å…³ï¼Œå¹¶ä¸”åœ¨æ ¼å¼å’Œæªè¾ä¸Šåƒä¸€ç¯‡çœŸå®çš„æ–‡æ¡£ç‰‡æ®µã€‚ã€é—®é¢˜ã€‘\n{question}\n\nã€è¯·ç”Ÿæˆä¸€ä¸ªå‡æƒ³çš„ã€ç”¨äºæ£€ç´¢çš„ç­”æ¡ˆã€‘"""

def fetch_internet_search_results(query: str, num_results: int = 5) -> List[Dict]:
    logging.info(f"ğŸŒ æ­£åœ¨æ‰§è¡Œè”ç½‘æœç´¢: '{query}'")
    search_results = []
    try:
        with DDGS() as ddgs:
            results = list(ddgs.text(query=query, region='wt-wt', safesearch='off', timelimit='y', max_results=num_results))
            urls = [r['href'] for r in results]
    except Exception as e:
        logging.error(f"è”ç½‘æœç´¢å¤±è´¥: {e}")
        return []
    def scrape_url(url: str):
        try:
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
            response = requests.get(url, timeout=10, headers=headers)
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                text = re.sub(r'\s+', ' ', soup.get_text()).strip()
                if text:
                    return {'name': f"Web: {url}", 'content': text}
        except Exception as e:
            logging.warning(f"çˆ¬å–URLå¤±è´¥: {url}, åŸå› : {e}")
        return None
    for url in urls:
        scraped_data = scrape_url(url)
        if scraped_data and scraped_data['content']:
            search_results.append(scraped_data)
            logging.info(f"âœ… æˆåŠŸçˆ¬å–: {url}")
    logging.info(f"ğŸŒ è”ç½‘æœç´¢å®Œæˆï¼Œè·å¾— {len(search_results)} ä¸ªæœ‰æ•ˆç½‘é¡µå†…å®¹ã€‚")
    return search_results

milvus_client = MilvusClient(host=MILVUS_HOST, port=MILVUS_PORT)

def execute_rag_pipeline_ray(files_data: List[Dict], query: str, use_hyde: bool) -> Dict:
    logging.info("ğŸš€ ======== å¼€å§‹æ‰§è¡ŒRAGå·¥ä½œæµ (Qwenæ¨¡å‹ç‰ˆ) ========")
    logging.info("æ­£åœ¨æœ¬åœ°å®ä¾‹åŒ–æ¨¡å‹...")
    embedding_model = EmbeddingActor()
    reranker_model = RerankerActor()
    llm_model = LLMActor()
    logging.info("âœ… æ¨¡å‹å®ä¾‹åŒ–å®Œæˆã€‚")
    parse_tasks = [parse_and_chunk_document.remote(f['content'], f['name']) for f in files_data]
    
    hypothetical_answer = ""
    if use_hyde:
        hyde_prompt = HYDE_PROMPT_TEMPLATE.format(question=query)
        hypothetical_answer = llm_model.generate(hyde_prompt).strip()
        retrieval_text = hypothetical_answer
    else:
        retrieval_text = query
    
    parsed_results = ray.get(parse_tasks)
    all_chunks_with_source = [chunk for result in parsed_results for chunk in result]
    if not all_chunks_with_source:
        return {"answer": "âŒ æœªèƒ½ä»ä»»ä½•æ–‡ä»¶ä¸­æå–æ–‡æœ¬å—ã€‚", "hypothetical_answer": "", "sources": []}
    all_chunk_texts = [chunk['content'] for chunk in all_chunks_with_source]
    
    current_query = query
    
    for attempt in range(MAX_OPTIMIZATION_ATTEMPTS + 1):
        logging.info(f"--- ç¬¬ {attempt + 1} æ¬¡å°è¯• ---")
        if attempt > 0:
            retrieval_text = current_query
        
        logging.info(f"å½“å‰ç”¨äºæ£€ç´¢çš„æ–‡æœ¬: '{retrieval_text[:100]}...'")
        query_vector = embedding_model.embed([retrieval_text])
        
        if attempt == 0:
            chunk_vectors = embedding_model.embed(all_chunk_texts)
            collection_name = f"rag_session_{datetime.now().strftime('%Y%m%d%H%M%S')}"
            milvus_client.insert(collection_name, all_chunk_texts, chunk_vectors)
        
        retrieved_docs = milvus_client.search(collection_name, query_vector)
        # rerank
        if retrieved_docs:
            logging.info(f"åˆæ­¥æ£€ç´¢åˆ° {len(retrieved_docs)} ç¯‡æ–‡æ¡£ï¼Œæ­£åœ¨è¿›è¡Œé‡æ’åº...")
            rerank_pairs = [(current_query, doc) for doc in retrieved_docs]
            rerank_scores = reranker_model.compute_score(rerank_pairs)
            
            reranked_results = sorted(zip(rerank_scores, retrieved_docs), key=lambda x: x[0], reverse=True)
            
            top_k_reranked = 3
            final_docs = [doc for score, doc in reranked_results[:top_k_reranked]]
            logging.info(f"é‡æ’åºå®Œæˆï¼Œé€‰å‡º Top-{top_k_reranked} ç¯‡æœ€ç›¸å…³çš„æ–‡æ¡£ã€‚")

            if final_docs:
                relevant_docs_with_source = [chunk for chunk in all_chunks_with_source if chunk['content'] in final_docs]
                context_parts = [f"[æ¥æº: {doc['source']} (å—å·: {doc['chunk_index']})]\n{doc['content']}" for doc in relevant_docs_with_source]
                context = "\n---\n".join(context_parts)
                
                final_prompt = FINAL_ANSWER_TEMPLATE.format(question=query, context=context)
                final_response = llm_model.generate(final_prompt)
                
                logging.info("ğŸ ======== Ray RAG å·¥ä½œæµæ‰§è¡Œå®Œæ¯• ========")
                sources_used = sorted(list(set([doc['source'] for doc in relevant_docs_with_source])))
                return {"answer": final_response, "hypothetical_answer": hypothetical_answer, "sources": sources_used}
        
        if attempt < MAX_OPTIMIZATION_ATTEMPTS:
            logging.warning("æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œæ­£åœ¨å°è¯•ä¼˜åŒ–æŸ¥è¯¢...")
            optimization_prompt = QUERY_OPTIMIZATION_TEMPLATE.format(question=current_query)
            optimized_query = llm_model.generate(optimization_prompt).strip()
            if optimized_query and optimized_query != current_query:
                current_query = optimized_query
            else:
                break

    return {
        "answer": "æŠ±æ­‰ï¼Œåœ¨æ‚¨æä¾›çš„æ–‡æ¡£ä¸­ï¼Œæˆ‘å¤šæ¬¡å°è¯•åä»æœªæ‰¾åˆ°èƒ½å›ç­”æ‚¨é—®é¢˜çš„ç›¸å…³ä¿¡æ¯ã€‚",
        "hypothetical_answer": hypothetical_answer,
        "sources": []
    }

def run_streamlit_app():
    st.set_page_config(page_title="åˆ†å¸ƒå¼RAGåº”ç”¨ (Rayç‰ˆ)", layout="wide")
    st.title("ğŸš€ åˆ†å¸ƒå¼RAGåº”ç”¨-Qwen")
    st.markdown("ä¸Šä¼ æ–‡ä»¶ã€è¾“å…¥é—®é¢˜ï¼Œç³»ç»Ÿå°†é€šè¿‡ Ray åˆ†å¸ƒå¼åç«¯å¹¶è¡Œå¤„ç†æ•°æ®å¹¶ç”Ÿæˆå›ç­”ã€‚")
    if "response" not in st.session_state: st.session_state.response = "è¯·åœ¨ä¸‹æ–¹æäº¤é—®é¢˜å’Œæ–‡ä»¶..."
    if "hypothetical_answer" not in st.session_state: st.session_state.hypothetical_answer = ""
    if "sources" not in st.session_state: st.session_state.sources = []
    with st.sidebar:
        st.subheader("âš™ï¸ é«˜çº§é€‰é¡¹")
        use_hyde = st.toggle("å¯ç”¨HyDEç­–ç•¥", value=True)
    with st.form("rag_form"):
        query = st.text_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜:")
        uploaded_files = st.file_uploader("ä¸Šä¼ çŸ¥è¯†åº“æ–‡ä»¶", accept_multiple_files=True)
        col1, col2, _ = st.columns([1, 1, 3])
        with col1: submit_button = st.form_submit_button("ä»…æ–‡ä»¶é—®ç­”")
        with col2: submit_with_internet_button = st.form_submit_button("æ–‡ä»¶+è”ç½‘é—®ç­”")
    if submit_button or submit_with_internet_button:
        if query:
            all_files_data = []
            if uploaded_files:
                all_files_data.extend([{'name': f.name, 'content': f.getvalue()} for f in uploaded_files])
            if submit_with_internet_button:
                with st.spinner("æ­£åœ¨è¿›è¡Œè”ç½‘æœç´¢..."):
                    internet_data = fetch_internet_search_results(query)
                    internet_data_bytes = [
                        {'name': item['name'], 'content': item['content'].encode('utf-8')}
                        for item in internet_data
                    ]
                    all_files_data.extend(internet_data)
            if not all_files_data:
                st.error("é”™è¯¯ï¼šè¯·è‡³å°‘ä¸Šä¼ ä¸€ä¸ªæ–‡ä»¶æˆ–ä½¿ç”¨è”ç½‘åŠŸèƒ½ã€‚")
            else:
                with st.spinner("ç³»ç»Ÿæ­£åœ¨é€šè¿‡ Ray åˆ†å¸ƒå¼åç«¯å¤„ç†ä¸­..."):
                    try:
                        result_dict = execute_rag_pipeline_ray(files_data=all_files_data, query=query, use_hyde=use_hyde)
                        st.session_state.response = result_dict.get("answer", "æœªèƒ½è·å–å›ç­”ã€‚")
                        st.session_state.hypothetical_answer = result_dict.get("hypothetical_answer", "")
                        st.session_state.sources = result_dict.get("sources", [])
                    except Exception as e:
                        st.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
                        logging.error(f"Streamlit UIå±‚æ•è·åˆ°å¼‚å¸¸: {e}", exc_info=True)
        else:
            st.error("é”™è¯¯ï¼šè¯·ç¡®ä¿æ‚¨å·²è¾“å…¥é—®é¢˜ã€‚")
    if st.session_state.hypothetical_answer:
        with st.expander("ğŸ” æŸ¥çœ‹â€œæ…¢æ€è€ƒâ€è¿‡ç¨‹ (HyDEç”Ÿæˆçš„å‡æƒ³ç­”æ¡ˆ)"):
            st.info(st.session_state.hypothetical_answer)
    st.subheader("æ¨¡å‹çš„å›ç­”:")
    st.text_area("response_output", value=st.session_state.response, height=300, disabled=True, label_visibility="collapsed")
    if st.session_state.sources:
        st.subheader("ä¿¡æ¯æ¥æº:")
        for source in st.session_state.sources:
            st.info(f"ğŸ“„ {source}")

if __name__ == "__main__":
    run_streamlit_app()