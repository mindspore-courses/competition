# åŸºäºmindnlp rayåˆ†å¸ƒå¼æ¨ç†
import os
import time
import logging
import re
from datetime import datetime
from typing import List, Dict

import streamlit as st
import ray

from ddgs import DDGS
from bs4 import BeautifulSoup
import requests

from ray_tasks import EmbeddingActor, LLMActor, parse_and_chunk_document

# RAY,MinIO settings
RAY_ADDRESS = os.getenv("RAY_ADDRESS", "ray://127.0.0.1:10001")
MINIO_HOST = os.getenv("MINIO_HOST", "minio:9000")
MINIO_ACCESS_KEY = os.getenv("MINIO_ACCESS_KEY", "minioadmin")
MINIO_SECRET_KEY = os.getenv("MINIO_SECRET_KEY", "minioadmin")
MILVUS_HOST = os.getenv("MILVUS_HOST", "standalone")
MILVUS_PORT = os.getenv("MILVUS_PORT", "19530")
MINIO_BUCKET_NAME = "rag-documents"
MAX_OPTIMIZATION_ATTEMPTS = 2

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


# 1.ray init
try:
    if not ray.is_initialized():
        logging.info(f"æ­£åœ¨è¿æ¥åˆ° Ray é›†ç¾¤: {RAY_ADDRESS}")
        ray.init(address=RAY_ADDRESS, ignore_reinit_error=True)
        logging.info("âœ… Ray è¿æ¥æˆåŠŸ!")
except Exception as e:
    logging.error(f"âŒ æ— æ³•è¿æ¥åˆ° Ray é›†ç¾¤: {e}")
    st.error(f"ä¸¥é‡é”™è¯¯ï¼šæ— æ³•è¿æ¥åˆ° Ray è®¡ç®—é›†ç¾¤ï¼Œè¯·æ£€æŸ¥ Ray Head æœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œã€‚é”™è¯¯è¯¦æƒ…: {e}")
    st.stop()



# 2. MilvusClient
class MilvusClient:
    def __init__(self, host, port):
        from pymilvus import connections, utility, FieldSchema, CollectionSchema, DataType, Collection
        self.connections, self.utility, self.Collection = connections, utility, Collection
        self.DataType, self.FieldSchema, self.CollectionSchema = DataType, FieldSchema, CollectionSchema
        for i in range(5):
            try:
                self.connections.connect("default", host=host, port=port)
                logging.info("âœ… Milvus è¿æ¥æˆåŠŸã€‚")
                return
            except Exception as e:
                logging.warning(f"Milvus è¿æ¥å°è¯• {i+1}/5 å¤±è´¥... Error: {e}")
                time.sleep(3)
        raise ConnectionError("é”™è¯¯ï¼šå¤šæ¬¡å°è¯•åæ— æ³•è¿æ¥åˆ°Milvusã€‚")

    def create_or_get_collection(self, collection_name: str, dim: int = 768) -> 'Collection':
        if self.utility.has_collection(collection_name):
            return self.Collection(collection_name)
        fields = [
            self.FieldSchema(name="pk", dtype=self.DataType.VARCHAR, is_primary=True, auto_id=True, max_length=100),
            self.FieldSchema(name="text", dtype=self.DataType.VARCHAR, max_length=65535),
            self.FieldSchema(name="embedding", dtype=self.DataType.FLOAT_VECTOR, dim=dim)
        ]
        schema = self.CollectionSchema(fields, "RAGçŸ¥è¯†åº“é›†åˆ")
        collection = self.Collection(name=collection_name, schema=schema)
        index_params = {"index_type": "IVF_FLAT", "metric_type": "L2", "params": {"nlist": 1024}}
        collection.create_index(field_name="embedding", index_params=index_params)
        return collection

    def insert(self, collection_name: str, texts: List[str], vectors: List[List[float]]):
        collection = self.create_or_get_collection(collection_name)
        collection.insert([texts, vectors])
        collection.flush()

    def search(self, collection_name: str, query_vector: List[List[float]], top_k: int = 3) -> List[str]:
        if not self.utility.has_collection(collection_name):
            return ["é”™è¯¯ï¼šçŸ¥è¯†åº“é›†åˆä¸å­˜åœ¨ã€‚"]
        collection = self.Collection(collection_name)
        collection.load()
        search_params = {"metric_type": "L2", "params": {"nprobe": 10}}
        results = collection.search(data=query_vector, anns_field="embedding", param=search_params, limit=top_k, output_fields=["text"])
        return [hit.entity.get('text') for hit in results[0]] if results else []


# 3. RAG Prompt templates

RELEVANCE_ASSESSMENT_TEMPLATE = """ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£ç›¸å…³æ€§è¯„ä¼°å‘˜ã€‚è¯·åˆ¤æ–­ä¸‹é¢æä¾›çš„ã€æ–‡æ¡£ç‰‡æ®µã€‘æ˜¯å¦èƒ½å¸®åŠ©å›ç­”ã€ç”¨æˆ·é—®é¢˜ã€‘ã€‚
è¯·åªå›ç­”â€œæ˜¯â€æˆ–â€œå¦â€ã€‚

ã€ç”¨æˆ·é—®é¢˜ã€‘
{question}

ã€æ–‡æ¡£ç‰‡æ®µã€‘
---
{document}
---

ã€è¯¥æ–‡æ¡£æ˜¯å¦ç›¸å…³ï¼Ÿã€‘
"""

QUERY_OPTIMIZATION_TEMPLATE = """ä½ æ˜¯ä¸€ä¸ªæœç´¢å¼•æ“ä¼˜åŒ–ä¸“å®¶ã€‚å½“å‰çš„ç”¨æˆ·é—®é¢˜åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ£€ç´¢åˆ°ç›¸å…³çš„ç»“æœã€‚
è¯·ä½ æ¢ä¸€ä¸ªè§’åº¦ï¼Œä½¿ç”¨ä¸åŒçš„å…³é”®è¯æˆ–è¡¨è¾¾æ–¹å¼ï¼Œé‡æ–°ç”Ÿæˆä¸€ä¸ªä¸åŸé—®é¢˜æ„å›¾ç›¸åŒï¼Œä½†å¯èƒ½æ›´å®¹æ˜“åœ¨æ•°æ®åº“ä¸­åŒ¹é…åˆ°å†…å®¹çš„æ–°é—®é¢˜ã€‚
è¯·åªæä¾›ä¼˜åŒ–åçš„æ–°é—®é¢˜ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€‚

ã€åŸå§‹é—®é¢˜ã€‘
{question}

ã€ä¼˜åŒ–åçš„æ–°é—®é¢˜ã€‘
"""

FINAL_ANSWER_TEMPLATE = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€ä¸¥è°¨çš„é—®ç­”åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä¸‹é¢æä¾›çš„ã€å¯å‚è€ƒçš„ä¸Šä¸‹æ–‡ã€‘æ¥å›ç­”ç”¨æˆ·çš„ã€é—®é¢˜ã€‘ã€‚
ä½ çš„å›ç­”å¿…é¡»éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š
1.  å®Œå…¨åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡è¿›è¡Œå›ç­”ï¼Œç¦æ­¢ä½¿ç”¨ä»»ä½•å¤–éƒ¨çŸ¥è¯†æˆ–è¿›è¡ŒçŒœæµ‹ã€‚
2.  åœ¨å›ç­”ä¸­ï¼Œä½ å¿…é¡»æ˜ç¡®å¼•ç”¨ä¿¡æ¯æ¥æºã€‚å¼•ç”¨æ ¼å¼ä¸ºï¼š[æ¥æº: æ–‡ä»¶å (å—å·: X)]ã€‚
3.  å¦‚æœä¸Šä¸‹æ–‡å†…å®¹è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·æ¸…æ™°ã€å‡†ç¡®åœ°ç»„ç»‡ç­”æ¡ˆã€‚
4.  å¦‚æœä¸Šä¸‹æ–‡å†…å®¹ä¸ç›¸å…³æˆ–ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºï¼šâ€œæ ¹æ®æ‚¨æä¾›çš„æ–‡æ¡£ï¼Œæˆ‘æ— æ³•æ‰¾åˆ°å…³äºè¿™ä¸ªé—®é¢˜çš„ç¡®åˆ‡ä¿¡æ¯ã€‚â€
5.  å›ç­”æ—¶è¯·ä¿æŒå®¢è§‚ã€ä¸“ä¸šçš„å£å»ï¼Œå¹¶ä¸”æ€»æ˜¯ä½¿ç”¨ä¸­æ–‡ã€‚

ã€å¯å‚è€ƒçš„ä¸Šä¸‹æ–‡ã€‘
---
{context}
---

ã€é—®é¢˜ã€‘
{question}

ã€ä½ çš„å›ç­”ã€‘
"""

HYDE_PROMPT_TEMPLATE = """ä½ æ˜¯ä¸€ä¸ªå–„äºå›ç­”é—®é¢˜çš„åŠ©æ‰‹ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„ã€é—®é¢˜ã€‘ï¼Œç”Ÿæˆä¸€ä¸ªè¯¦ç»†ã€å®Œæ•´ã€çœ‹èµ·æ¥éå¸¸ä¸“ä¸šçš„å›ç­”ã€‚
é‡è¦æç¤ºï¼šè¿™ä¸ªå›ç­”æ˜¯ç”¨äºåç»­æ£€ç´¢çš„ï¼Œæ‰€ä»¥å®ƒä¸éœ€è¦ä¿è¯äº‹å®çš„ç»å¯¹æ­£ç¡®æ€§ï¼Œä½†å¿…é¡»ä¸é—®é¢˜é«˜åº¦ç›¸å…³ï¼Œå¹¶ä¸”åœ¨æ ¼å¼å’Œæªè¾ä¸Šåƒä¸€ç¯‡çœŸå®çš„æ–‡æ¡£ç‰‡æ®µã€‚

ã€é—®é¢˜ã€‘
{question}

ã€è¯·ç”Ÿæˆä¸€ä¸ªå‡æƒ³çš„ã€ç”¨äºæ£€ç´¢çš„ç­”æ¡ˆã€‘
"""

def fetch_internet_search_results(query: str, num_results: int = 5) -> List[Dict]:
    """ä½¿ç”¨DuckDuckGoçš„APIè¿›è¡Œæœç´¢ï¼Œå¹¶çˆ¬å–å‰Nä¸ªç»“æœçš„æ–‡æœ¬å†…å®¹ã€‚"""
    logging.info(f"ğŸŒ æ­£åœ¨æ‰§è¡Œè”ç½‘æœç´¢: '{query}'")
    search_results = []
    
    try:
        with DDGS() as ddgs:
            results = list(ddgs.text(
                query=query,
                region='wt-wt', 
                safesearch='off', 
                timelimit='y', 
                max_results=num_results
            ))
            urls = [r['href'] for r in results]
    except Exception as e:
        logging.error(f"è”ç½‘æœç´¢å¤±è´¥: {e}")
        return []

    def scrape_url(url: str):
        """çˆ¬å–å•ä¸ªURLçš„æ–‡æœ¬å†…å®¹ã€‚"""
        try:
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
            response = requests.get(url, timeout=10, headers=headers)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                text = re.sub(r'\s+', ' ', soup.get_text()).strip()
                if text:
                    return {'name': f"Web: {url}", 'content': text}
        except requests.RequestException as e:
            logging.warning(f"çˆ¬å–URLå¤±è´¥: {url}, åŸå› : {e}")
        except Exception as e:
            logging.warning(f"å¤„ç†URLæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {url}, åŸå› : {e}")
        return None
    for url in urls:
        scraped_data = scrape_url(url)
        if scraped_data and scraped_data['content']:
            search_results.append(scraped_data)
            logging.info(f"âœ… æˆåŠŸçˆ¬å–: {url}")

    logging.info(f"ğŸŒ è”ç½‘æœç´¢å®Œæˆï¼Œè·å¾— {len(search_results)} ä¸ªæœ‰æ•ˆç½‘é¡µå†…å®¹ã€‚")
    logging.info(search_results[:2])
    return search_results


# 5. rag pipeline

def execute_rag_pipeline_ray(files_data: List[Dict], query: str, use_hyde: bool) -> Dict:
    logging.info("ğŸš€ ======== å¼€å§‹æ‰§è¡ŒRAGå·¥ä½œæµ ========")
    
    try:
        embedding_actor = ray.get_actor("EmbeddingActor")
        llm_actor = ray.get_actor("LLMActor")
        logging.info("âœ… Actor å¥æŸ„è·å–æˆåŠŸã€‚")
    except ValueError:
        logging.warning("Actor æœªæ‰¾åˆ°ï¼Œæ­£åœ¨åˆ›å»ºæ–°çš„ Actor å®ä¾‹...")
        embedding_actor = EmbeddingActor.options(name="EmbeddingActor", get_if_exists=True).remote()
        llm_actor = LLMActor.options(name="LLMActor", get_if_exists=True).remote()
        logging.info("âœ… æ–°çš„ Actor å®ä¾‹å·²åˆ›å»ºã€‚")

    parse_tasks = [parse_and_chunk_document.remote(f['content'], f['name']) for f in files_data]
    logging.info(f"æäº¤äº† {len(parse_tasks)} ä¸ªæ–‡ä»¶è§£æä»»åŠ¡åˆ° Rayã€‚")
    
    hypothetical_answer_ref = None
    if use_hyde:
        logging.info("ğŸ’¡ HyDEç­–ç•¥å·²å¯ç”¨ï¼Œæ­£åœ¨ç”Ÿæˆå‡æƒ³ç­”æ¡ˆ...")
        hyde_prompt = HYDE_PROMPT_TEMPLATE.format(question=query)
        hypothetical_answer_ref = llm_actor.generate.remote(hyde_prompt)

    parsed_results = ray.get(parse_tasks)
    all_chunks_with_source = [chunk for result in parsed_results for chunk in result]
    
    if not all_chunks_with_source:
        return {"answer": "âŒ æœªèƒ½ä»ä»»ä½•æ–‡ä»¶ä¸­æå–æ–‡æœ¬å—ã€‚", "hypothetical_answer": "", "sources": []}

    all_chunk_texts = [chunk['content'] for chunk in all_chunks_with_source]
    logging.info(f"æ‰€æœ‰æ–‡ä»¶è§£æå®Œæˆï¼Œå…±å¾—åˆ° {len(all_chunk_texts)} ä¸ªæ–‡æœ¬å—ã€‚:{all_chunk_texts[:2]}...")
    
    hypothetical_answer = ""
    if use_hyde and hypothetical_answer_ref:
        hypothetical_answer = ray.get(hypothetical_answer_ref).strip()
        logging.info(f"ğŸ“ ç”Ÿæˆçš„å‡æƒ³ç­”æ¡ˆ: '{hypothetical_answer[:100]}...'")
        retrieval_text = hypothetical_answer
    else:
        retrieval_text = query

    current_query = query
    
    for attempt in range(MAX_OPTIMIZATION_ATTEMPTS + 1):
        logging.info(f"--- ç¬¬ {attempt + 1} æ¬¡å°è¯• ---")
        
        if attempt > 0:
            retrieval_text = current_query

        logging.info(f"å½“å‰ç”¨äºæ£€ç´¢çš„æ–‡æœ¬: '{retrieval_text[:100]}...'")
        
        query_vector_ref = embedding_actor.embed.remote([retrieval_text])
        chunk_vectors_ref = embedding_actor.embed.remote(all_chunk_texts)
        query_vector, chunk_vectors = ray.get([query_vector_ref, chunk_vectors_ref])
        
        if not chunk_vectors or not query_vector:
            return {"answer": "âŒ å‘é‡åŒ–å¤±è´¥ã€‚", "hypothetical_answer": hypothetical_answer, "sources": []}
        
        if attempt == 0:
            collection_name = f"rag_session_{datetime.now().strftime('%Y%m%d%H%M%S')}"
            milvus_client.insert(collection_name, all_chunk_texts, chunk_vectors)
            logging.info(f"å·²å°† {len(all_chunk_texts)} ä¸ªå‘é‡å­˜å…¥ Milvus é›†åˆ '{collection_name}'ã€‚")

        retrieved_docs = milvus_client.search(collection_name, query_vector)
        
        if retrieved_docs:
            assessment_tasks = [
                llm_actor.generate.remote(
                    RELEVANCE_ASSESSMENT_TEMPLATE.format(question=current_query, document=doc)
                ) for doc in retrieved_docs
            ]
            assessment_results = ray.get(assessment_tasks)
            
            relevant_docs_texts = [doc for doc, assessment in zip(retrieved_docs, assessment_results) if "æ˜¯" in assessment.strip()]
            logging.info(f"æ£€ç´¢åˆ° {len(retrieved_docs)} ç¯‡æ–‡æ¡£ï¼Œå…¶ä¸­ {len(relevant_docs_texts)} ç¯‡é€šè¿‡ç›¸å…³æ€§è¯„ä¼°ã€‚")

            if relevant_docs_texts:
                relevant_docs_with_source = [
                    chunk for chunk in all_chunks_with_source if chunk['content'] in relevant_docs_texts
                ]
                
                context_parts = []
                for doc in relevant_docs_with_source:
                    source_tag = f"[æ¥æº: {doc['source']} (å—å·: {doc['chunk_index']})]"
                    context_parts.append(f"{source_tag}\n{doc['content']}")
                context = "\n---\n".join(context_parts)
                
                final_prompt = FINAL_ANSWER_TEMPLATE.format(question=query, context=context)
                logging.info("æäº¤æœ€ç»ˆç­”æ¡ˆç”Ÿæˆä»»åŠ¡ã€‚")
                final_response = ray.get(llm_actor.generate.remote(final_prompt))
                logging.info("ğŸ ======== Ray RAG å·¥ä½œæµæ‰§è¡Œå®Œæ¯• ========")
                
                sources_used = sorted(list(set([doc['source'] for doc in relevant_docs_with_source])))
                return {"answer": final_response, "hypothetical_answer": hypothetical_answer, "sources": sources_used}
        
        if attempt < MAX_OPTIMIZATION_ATTEMPTS:
            logging.warning("æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œæ­£åœ¨å°è¯•ä¼˜åŒ–æŸ¥è¯¢...")
            optimization_prompt = QUERY_OPTIMIZATION_TEMPLATE.format(question=current_query)
            optimized_query = ray.get(llm_actor.generate.remote(optimization_prompt)).strip()
            if optimized_query and optimized_query != current_query:
                current_query = optimized_query
            else:
                logging.error("æŸ¥è¯¢ä¼˜åŒ–å¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆæ–°çš„æŸ¥è¯¢ã€‚")
                break
        else:
            logging.warning("å·²è¾¾åˆ°æœ€å¤§ä¼˜åŒ–æ¬¡æ•°ã€‚")

    return {
        "answer": "æŠ±æ­‰ï¼Œåœ¨æ‚¨æä¾›çš„æ–‡æ¡£ä¸­ï¼Œæˆ‘å¤šæ¬¡å°è¯•åä»æœªæ‰¾åˆ°èƒ½å›ç­”æ‚¨é—®é¢˜çš„ç›¸å…³ä¿¡æ¯ã€‚",
        "hypothetical_answer": hypothetical_answer,
        "sources": []
    }


# 6. Streamlit

def run_streamlit_app():
    st.set_page_config(page_title="åˆ†å¸ƒå¼RAGåº”ç”¨ (Rayç‰ˆ)", layout="wide")
    st.title("ğŸš€ åˆ†å¸ƒå¼RAGåº”ç”¨ (Ray ç»Ÿä¸€è®¡ç®—åç«¯)")
    st.markdown("ä¸Šä¼ æ–‡ä»¶ã€è¾“å…¥é—®é¢˜ï¼Œå¯é€‰è”ç½‘æœç´¢ï¼Œç³»ç»Ÿå°†é€šè¿‡ Ray åˆ†å¸ƒå¼åç«¯å¹¶è¡Œå¤„ç†æ•°æ®å¹¶ç”Ÿæˆå›ç­”ã€‚")

    if "response" not in st.session_state:
        st.session_state.response = "è¯·åœ¨ä¸‹æ–¹æäº¤é—®é¢˜å’Œæ–‡ä»¶ï¼Œæˆ‘ä¼šåœ¨è¿™é‡Œç»™å‡ºå›ç­”..."
    if "hypothetical_answer" not in st.session_state:
        st.session_state.hypothetical_answer = ""
    if "sources" not in st.session_state:
        st.session_state.sources = []

    with st.sidebar:
        st.subheader("âš™ï¸ é«˜çº§é€‰é¡¹")
        use_hyde = st.toggle("å¯ç”¨HyDEç­–ç•¥", value=True, help="é€šè¿‡ç”Ÿæˆå‡æƒ³ç­”æ¡ˆæ¥ä¼˜åŒ–æ£€ç´¢ï¼Œå¯èƒ½æå‡ç›¸å…³æ€§ä½†ä¼šå¢åŠ å°‘é‡å»¶è¿Ÿã€‚")

    with st.form("rag_form"):
        query = st.text_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜:", placeholder="ä¾‹å¦‚ï¼šè¿™ä»½æ–‡æ¡£çš„æ ¸å¿ƒå†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ")
        
        uploaded_files = st.file_uploader(
            "ä¸Šä¼ çŸ¥è¯†åº“æ–‡ä»¶ï¼ˆæ”¯æŒDocx, PPTX, Csv, PDF, MD, Txt, å›¾ç‰‡, è¯­éŸ³ï¼‰",
            accept_multiple_files=True,
            type=['docx', 'pptx', 'csv', 'pdf', 'md', 'txt', 'png', 'jpg', 'jpeg', 'wav', 'mp3', 'm4a']
        )
        
        col1, col2, _ = st.columns([1, 1, 3])
        with col1:
            submit_button = st.form_submit_button("ä»…æ–‡ä»¶é—®ç­”")
        with col2:
            submit_with_internet_button = st.form_submit_button("æ–‡ä»¶+è”ç½‘é—®ç­”")


    if submit_button or submit_with_internet_button:
        if query:
            all_files_data = []
            if uploaded_files:
                all_files_data.extend([{'name': f.name, 'content': f.getvalue()} for f in uploaded_files])
            
            if submit_with_internet_button:
                with st.spinner("æ­£åœ¨è¿›è¡Œè”ç½‘æœç´¢å¹¶çˆ¬å–å†…å®¹..."):
                    internet_data = fetch_internet_search_results(query)
                    internet_data_bytes = [
                        {'name': item['name'], 'content': item['content'].encode('utf-8')}
                        for item in internet_data
                    ]
                    all_files_data.extend(internet_data)
            
            if not all_files_data:
                st.error("é”™è¯¯ï¼šè¯·è‡³å°‘ä¸Šä¼ ä¸€ä¸ªæ–‡ä»¶æˆ–ä½¿ç”¨è”ç½‘åŠŸèƒ½ã€‚")
            else:
                with st.spinner("ç³»ç»Ÿæ­£åœ¨é€šè¿‡ Ray åˆ†å¸ƒå¼åç«¯å¤„ç†ä¸­..."):
                    try:
                        logging.info(f"Streamlit æ¥æ”¶åˆ°æŸ¥è¯¢: '{query}' å’Œ {len(all_files_data)} ä¸ªæ•°æ®æºã€‚")
                        result_dict = execute_rag_pipeline_ray(files_data=all_files_data, query=query, use_hyde=use_hyde)
                        
                        st.session_state.response = result_dict.get("answer", "æœªèƒ½è·å–å›ç­”ã€‚")
                        st.session_state.hypothetical_answer = result_dict.get("hypothetical_answer", "")
                        st.session_state.sources = result_dict.get("sources", [])
                    except Exception as e:
                        error_message = f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}"
                        st.error(error_message)
                        st.session_state.response = error_message
                        logging.error(f"Streamlit UIå±‚æ•è·åˆ°å¼‚å¸¸: {e}", exc_info=True)
        else:
            st.error("é”™è¯¯ï¼šè¯·ç¡®ä¿æ‚¨å·²è¾“å…¥é—®é¢˜ã€‚")

    if st.session_state.hypothetical_answer:
        with st.expander("ğŸ” æŸ¥çœ‹â€œæ…¢æ€è€ƒâ€è¿‡ç¨‹ (HyDEç”Ÿæˆçš„å‡æƒ³ç­”æ¡ˆ)"):
            st.info(st.session_state.hypothetical_answer)

    st.subheader("æ¨¡å‹çš„å›ç­”:")
    st.text_area("response_output", value=st.session_state.response, height=300, disabled=True, label_visibility="collapsed")

    if st.session_state.sources:
        st.subheader("ä¿¡æ¯æ¥æº:")
        for source in st.session_state.sources:
            st.info(f"ğŸ“„ {source}")

if __name__ == "__main__":
    run_streamlit_app()